# serve-mistral-nemo-as-trt
A guide on how to compile mistral-nemo to a TensorRT-LLM engine and serve it on Triton-Inference-Server
